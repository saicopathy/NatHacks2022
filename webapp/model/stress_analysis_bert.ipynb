{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "isRzUCY2NatY",
        "outputId": "f28d9f9e-63d1-4f8d-940a-1b98b7d5077e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.21.6)\n",
            "Collecting matplotlib==3.0.3\n",
            "  Downloading matplotlib-3.0.3-cp37-cp37m-manylinux1_x86_64.whl (13.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.7)\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 23 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.12.0)\n",
            "Collecting bert-tensorflow==1.0.1\n",
            "  Downloading bert_tensorflow-1.0.1-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.5.0)\n",
            "Collecting bentoml\n",
            "  Downloading bentoml-1.0.2-py3-none-any.whl (774 kB)\n",
            "\u001b[K     |████████████████████████████████| 774 kB 75.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (4.64.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 3)) (1.4.4)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (1.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (1.47.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 77.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (1.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15->-r requirements.txt (line 7)) (0.37.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.3->-r requirements.txt (line 3)) (4.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 7)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15->-r requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2022.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r requirements.txt (line 5)) (5.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->-r requirements.txt (line 6)) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->-r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud->-r requirements.txt (line 10)) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bentoml->-r requirements.txt (line 11)) (2.23.0)\n",
            "Collecting opentelemetry-instrumentation-aiohttp-client==0.28b0\n",
            "  Downloading opentelemetry_instrumentation_aiohttp_client-0.28b0-py3-none-any.whl (11 kB)\n",
            "Collecting PyYAML>=5.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 74.0 MB/s \n",
            "\u001b[?25hCollecting watchfiles>=0.15.0\n",
            "  Downloading watchfiles-0.16.1-cp37-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from bentoml->-r requirements.txt (line 11)) (3.8.1)\n",
            "Collecting python-dotenv>=0.20.0\n",
            "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
            "Collecting pathspec\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting prometheus-client<0.14.0,>=0.10.0\n",
            "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting rich>=11.2.0\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 77.2 MB/s \n",
            "\u001b[?25hCollecting pip-tools>=6.6.2\n",
            "  Downloading pip_tools-6.8.0-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 310 kB/s \n",
            "\u001b[?25hCollecting opentelemetry-sdk==1.9.0\n",
            "  Downloading opentelemetry_sdk-1.9.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting schema\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from bentoml->-r requirements.txt (line 11)) (5.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from bentoml->-r requirements.txt (line 11)) (21.3)\n",
            "Collecting Jinja2>=3.0.1\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 78.6 MB/s \n",
            "\u001b[?25hCollecting backports.cached-property\n",
            "  Downloading backports.cached_property-1.0.2-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from bentoml->-r requirements.txt (line 11)) (1.3.0)\n",
            "Collecting fs\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting circus\n",
            "  Downloading circus-0.17.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 24.3 MB/s \n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.28b0\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.28b0-py3-none-any.whl (7.5 kB)\n",
            "Collecting opentelemetry-api==1.9.0\n",
            "  Downloading opentelemetry_api-1.9.0-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting deepmerge\n",
            "  Downloading deepmerge-1.0.1-py3-none-any.whl (8.0 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.28b0\n",
            "  Downloading opentelemetry_semantic_conventions-0.28b0-py3-none-any.whl (23 kB)\n",
            "Collecting simple-di>=0.1.4\n",
            "  Downloading simple_di-0.1.5-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: attrs>=21.1.0 in /usr/local/lib/python3.7/dist-packages (from bentoml->-r requirements.txt (line 11)) (21.4.0)\n",
            "Collecting pynvml<12\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting starlette\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.18.2-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting opentelemetry-util-http==0.28b0\n",
            "  Downloading opentelemetry_util_http-0.28b0-py3-none-any.whl (6.1 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting opentelemetry-instrumentation==0.28b0\n",
            "  Downloading opentelemetry_instrumentation-0.28b0-py3-none-any.whl (22 kB)\n",
            "Collecting cattrs>=22.1.0\n",
            "  Downloading cattrs-22.1.0-py3-none-any.whl (33 kB)\n",
            "Collecting Deprecated>=1.2.6\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting asgiref~=3.0\n",
            "  Downloading asgiref-3.5.2-py3-none-any.whl (22 kB)\n",
            "Collecting exceptiongroup\n",
            "  Downloading exceptiongroup-1.0.0rc8-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0.1->bentoml->-r requirements.txt (line 11)) (2.0.1)\n",
            "Collecting pip>=21.2\n",
            "  Downloading pip-22.2.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 64.7 MB/s \n",
            "\u001b[?25hCollecting build\n",
            "  Downloading build-0.8.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=11.2.0->bentoml->-r requirements.txt (line 11)) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting anyio<4,>=3.0.0\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.0.0->watchfiles>=0.15.0->bentoml->-r requirements.txt (line 11)) (2.10)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->bentoml->-r requirements.txt (line 11)) (2.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->bentoml->-r requirements.txt (line 11)) (1.7.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->bentoml->-r requirements.txt (line 11)) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->bentoml->-r requirements.txt (line 11)) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->bentoml->-r requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->bentoml->-r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->bentoml->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from build->pip-tools>=6.6.2->bentoml->-r requirements.txt (line 11)) (2.0.1)\n",
            "Requirement already satisfied: pep517>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from build->pip-tools>=6.6.2->bentoml->-r requirements.txt (line 11)) (0.12.0)\n",
            "Requirement already satisfied: pyzmq>=17.0 in /usr/local/lib/python3.7/dist-packages (from circus->bentoml->-r requirements.txt (line 11)) (23.2.0)\n",
            "Requirement already satisfied: tornado>=5.0.2 in /usr/local/lib/python3.7/dist-packages (from circus->bentoml->-r requirements.txt (line 11)) (5.1.1)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.7/dist-packages (from fs->bentoml->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bentoml->-r requirements.txt (line 11)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bentoml->-r requirements.txt (line 11)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bentoml->-r requirements.txt (line 11)) (2022.6.15)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from schema->bentoml->-r requirements.txt (line 11)) (0.5.5)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gast, python-multipart\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=9cc79a094031fcfc7c602be3658a6a804ef09b80e6ac661781facaa5d71b3714\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=2b9e376263d7bde43cc2bb9bdf92294f30643b68b6e9d3bd1bd9275a83d5a3ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built gast python-multipart\n",
            "Installing collected packages: Deprecated, sniffio, opentelemetry-api, pip, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-instrumentation, h11, exceptiongroup, commonmark, build, asgiref, anyio, watchfiles, uvicorn, tensorflow-estimator, tensorboard, starlette, simple-di, schema, rich, PyYAML, python-multipart, python-dotenv, pynvml, prometheus-client, pip-tools, pathspec, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-aiohttp-client, keras-applications, Jinja2, gast, fs, deepmerge, circus, cattrs, backports.cached-property, tensorflow, matplotlib, bert-tensorflow, bentoml\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus-client 0.14.1\n",
            "    Uninstalling prometheus-client-0.14.1:\n",
            "      Successfully uninstalled prometheus-client-0.14.1\n",
            "  Attempting uninstall: pip-tools\n",
            "    Found existing installation: pip-tools 6.2.0\n",
            "    Uninstalling pip-tools-6.2.0:\n",
            "      Successfully uninstalled pip-tools-6.2.0\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "plotnine 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 3.0.3 which is incompatible.\n",
            "mizani 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 3.0.3 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Deprecated-1.2.13 Jinja2-3.1.2 PyYAML-6.0 anyio-3.6.1 asgiref-3.5.2 backports.cached-property-1.0.2 bentoml-1.0.2 bert-tensorflow-1.0.1 build-0.8.0 cattrs-22.1.0 circus-0.17.1 commonmark-0.9.1 deepmerge-1.0.1 exceptiongroup-1.0.0rc8 fs-2.4.16 gast-0.2.2 h11-0.13.0 keras-applications-1.0.8 matplotlib-3.0.3 opentelemetry-api-1.9.0 opentelemetry-instrumentation-0.28b0 opentelemetry-instrumentation-aiohttp-client-0.28b0 opentelemetry-instrumentation-asgi-0.28b0 opentelemetry-sdk-1.9.0 opentelemetry-semantic-conventions-0.28b0 opentelemetry-util-http-0.28b0 pathspec-0.9.0 pip-22.2.1 pip-tools-6.8.0 prometheus-client-0.13.1 pynvml-11.4.1 python-dotenv-0.20.0 python-multipart-0.0.5 rich-12.5.1 schema-0.7.5 simple-di-0.1.5 sniffio-1.2.0 starlette-0.20.4 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 uvicorn-0.18.2 watchfiles-0.16.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n8k6nKL57J4",
        "outputId": "1981f647-b732-4d8d-e75a-a7d329c80d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!python config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTjOWW_j6FUo",
        "outputId": "c315c08e-2e39-4e0e-fa02-70470350d1d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu==1.15\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.5/411.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.37.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.47.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.12.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.1.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install tensorflow-gpu==1.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e__1thv7UKLl"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nftRWf4UvtA",
        "outputId": "1636271b-ea83-438a-8624-eaea4a83d35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 3160872522097742575\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2097244281006896592\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 9276447438144457617\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15964005991\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 3522835478767124150\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNoRPwAsU4ht",
        "outputId": "dee07f3e-a0dd-489c-8f0f-48394817a108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import bert\n",
        "# import python modules defined by BERT\n",
        "from bert import optimization\n",
        "from bert import run_classifier\n",
        "from bert import tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "05oZ97UH4anj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2KEpHmmb4dgg"
      },
      "outputs": [],
      "source": [
        "import logging, sys\n",
        "logging.disable(sys.maxsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ccCdKTFsU6XT"
      },
      "outputs": [],
      "source": [
        "## import dataset\n",
        "\n",
        "train = pd.read_csv('dreaddit-train.csv', encoding = \"ISO-8859-1\")\n",
        "test = pd.read_csv('dreaddit-test.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "DATA_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yx5LHJySU_mS"
      },
      "outputs": [],
      "source": [
        "# transform dataset into a format understood by BERT\n",
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gVH1UGmnVCll"
      },
      "outputs": [],
      "source": [
        "# Load a vocabulary file and lowercasing information directly from the BERT tf hub module\n",
        "\n",
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    with tf.Graph().as_default():\n",
        "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "        with tf.Session() as sess:\n",
        "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                                tokenization_info[\"do_lower_case\"]])\n",
        "\n",
        "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, \n",
        "                                    do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aM_gLhobVECI"
      },
      "outputs": [],
      "source": [
        "# Set the maximum sequence length. \n",
        "def get_max_len(text):\n",
        "    max_len = 0\n",
        "    for i in range(len(train)):\n",
        "        if len(text.iloc[i]) > max_len:\n",
        "            max_len = len(text.iloc[i])\n",
        "    return max_len\n",
        "\n",
        "temp = train.text.str.split(' ')\n",
        "max_len = get_max_len(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sUw6UcmEVGvl"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = max_len\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, \n",
        "                                                                  label_list, \n",
        "                                                                  MAX_SEQ_LENGTH, \n",
        "                                                                  tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, \n",
        "                                                                 label_list, \n",
        "                                                                 MAX_SEQ_LENGTH, \n",
        "                                                                 tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Zu6yYnakVHRH"
      },
      "outputs": [],
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "    \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
        "    bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
        "    bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "\n",
        "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "    # Use \"sequence_outputs\" for token-level output.\n",
        "    output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "    hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "    # Create our own layer to tune for politeness data.\n",
        "    output_weights = tf.get_variable(\n",
        "        \"output_weights\", [num_labels, hidden_size],\n",
        "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "    output_bias = tf.get_variable(\"output_bias\", \n",
        "                                  [num_labels], \n",
        "                                  initializer=tf.zeros_initializer())\n",
        "\n",
        "    with tf.variable_scope(\"loss\"):\n",
        "\n",
        "        # Dropout helps prevent overfitting\n",
        "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "        logits = tf.nn.bias_add(logits, output_bias)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "        # Convert labels into one-hot encoding\n",
        "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "        if is_predicting:\n",
        "            return (predicted_labels, log_probs)\n",
        "\n",
        "        # If we're train/eval, compute loss between predicted and actual label\n",
        "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "        loss = tf.reduce_mean(per_example_loss)\n",
        "        return (loss, predicted_labels, log_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "02_MEm_nVJWH"
      },
      "outputs": [],
      "source": [
        "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
        "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "        input_ids = features[\"input_ids\"]\n",
        "        input_mask = features[\"input_mask\"]\n",
        "        segment_ids = features[\"segment_ids\"]\n",
        "        label_ids = features[\"label_ids\"]\n",
        "\n",
        "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "        # TRAIN and EVAL\n",
        "        if not is_predicting:\n",
        "            (loss, predicted_labels, log_probs) = create_model(is_predicting, \n",
        "                                                               input_ids, \n",
        "                                                               input_mask, \n",
        "                                                               segment_ids, \n",
        "                                                               label_ids, \n",
        "                                                               num_labels)\n",
        "\n",
        "            train_op = bert.optimization.create_optimizer(loss, \n",
        "                                                          learning_rate, \n",
        "                                                          num_train_steps, \n",
        "                                                          num_warmup_steps, \n",
        "                                                          use_tpu=False)\n",
        "\n",
        "            # Calculate evaluation metrics. \n",
        "            def metric_fn(label_ids, predicted_labels):\n",
        "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "                f1_score = tf.contrib.metrics.f1_score(label_ids, predicted_labels)\n",
        "                auc = tf.metrics.auc(label_ids, predicted_labels)\n",
        "                recall = tf.metrics.recall(label_ids, predicted_labels)\n",
        "                precision = tf.metrics.precision(label_ids, predicted_labels) \n",
        "                true_pos = tf.metrics.true_positives(label_ids, predicted_labels)\n",
        "                true_neg = tf.metrics.true_negatives(label_ids, predicted_labels)   \n",
        "                false_pos = tf.metrics.false_positives(label_ids, predicted_labels)  \n",
        "                false_neg = tf.metrics.false_negatives(label_ids, predicted_labels)\n",
        "\n",
        "                return {\n",
        "                    \"eval_accuracy\": accuracy,\n",
        "                    \"f1_score\": f1_score,\n",
        "                    \"auc\": auc,\n",
        "                    \"precision\": precision,\n",
        "                    \"recall\": recall,\n",
        "                    \"true_positives\": true_pos,\n",
        "                    \"true_negatives\": true_neg,\n",
        "                    \"false_positives\": false_pos,\n",
        "                    \"false_negatives\": false_neg\n",
        "                }\n",
        "\n",
        "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "            else:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
        "        else:\n",
        "            (predicted_labels, log_probs) = create_model(is_predicting, \n",
        "                                                         input_ids, \n",
        "                                                         input_mask, \n",
        "                                                         segment_ids, \n",
        "                                                         label_ids, \n",
        "                                                         num_labels)\n",
        "\n",
        "            predictions = {'probabilities': log_probs, 'labels': predicted_labels}\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "    # Return the actual model function in the closure\n",
        "    return model_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SO9ucusFVNDA"
      },
      "outputs": [],
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "BATCH_SIZE = 24\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "esLhVFl2VOfq"
      },
      "outputs": [],
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "POP3vttrVQTQ"
      },
      "outputs": [],
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "OUTPUT_DIR = 'output'\n",
        "\n",
        "run_config = tf.estimator.RunConfig(model_dir=OUTPUT_DIR,\n",
        "                                    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "                                    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "i2AInTJ1VVQi"
      },
      "outputs": [],
      "source": [
        "model_fn = model_fn_builder(num_labels=len(label_list), \n",
        "                            learning_rate=LEARNING_RATE,\n",
        "                            num_train_steps=num_train_steps,\n",
        "                            num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
        "                                   config=run_config,\n",
        "                                   params={\"batch_size\": BATCH_SIZE})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jZfKTOS2Vnez"
      },
      "outputs": [],
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(features=train_features,\n",
        "                                                      seq_length=MAX_SEQ_LENGTH,\n",
        "                                                      is_training=True,\n",
        "                                                      drop_remainder=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WikYpDz0V4cH",
        "outputId": "4e0e43e7-f37f-4d97-c4ea-6c75241283df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "Training took time  0:05:57.412266\n"
          ]
        }
      ],
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "\n",
        "# train the model \n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-pP8-fxGV6d6"
      },
      "outputs": [],
      "source": [
        "# check the test result\n",
        "test_input_fn = run_classifier.input_fn_builder(features=test_features,\n",
        "                                                seq_length=MAX_SEQ_LENGTH,\n",
        "                                                is_training=False,\n",
        "                                                drop_remainder=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5vWnL6rV-ku",
        "outputId": "7f14c14e-c7c0-49f9-c788-8a2e9af81163"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auc': 0.80218756,\n",
              " 'eval_accuracy': 0.8027972,\n",
              " 'f1_score': 0.81124485,\n",
              " 'false_negatives': 66.0,\n",
              " 'false_positives': 75.0,\n",
              " 'global_step': 354,\n",
              " 'loss': 0.74519736,\n",
              " 'precision': 0.8015873,\n",
              " 'recall': 0.8211382,\n",
              " 'true_negatives': 271.0,\n",
              " 'true_positives': 303.0}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HsuBVon8WEHi"
      },
      "outputs": [],
      "source": [
        "def predict(in_sentences):\n",
        "    labels = [\"non-stress\", \"stress\"]\n",
        "    labels_idx = [0, 1]\n",
        "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "\n",
        "    input_features = run_classifier.convert_examples_to_features(input_examples, \n",
        "                                                                 labels_idx, \n",
        "                                                                 MAX_SEQ_LENGTH, \n",
        "                                                                 tokenizer)\n",
        "    \n",
        "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, \n",
        "                                                       seq_length=MAX_SEQ_LENGTH, \n",
        "                                                       is_training=False, \n",
        "                                                       drop_remainder=False)\n",
        "\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "    return [{\"text\": sentence, \"confidence\": list(prediction['probabilities']), \"labels\": labels[prediction['labels']]}\n",
        "            for sentence, prediction in zip(in_sentences, predictions)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-jodbf0eGP7k"
      },
      "outputs": [],
      "source": [
        "pred_sentences = [\"It's Friday! We wish you a nice start into the weekend!\",\n",
        "\"Deep breathing exercises are very relaxing. It can also relieve the symptoms of stress and anxiety.\",\n",
        "\"Do you like fruits? I like so much! Be Happy, Keep Smiling!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k77Ofc3IGUCF",
        "outputId": "751b98ea-6b7f-44a5-dac3-3e5cca5abb31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'confidence': [-0.0051890453, -5.2637887],\n",
              "  'labels': 'non-stress',\n",
              "  'text': \"It's Friday! We wish you a nice start into the weekend!\"},\n",
              " {'confidence': [-0.0040529976, -5.510336],\n",
              "  'labels': 'non-stress',\n",
              "  'text': 'Deep breathing exercises are very relaxing. It can also relieve the symptoms of stress and anxiety.'},\n",
              " {'confidence': [-0.0058140685, -5.150384],\n",
              "  'labels': 'non-stress',\n",
              "  'text': 'Do you like fruits? I like so much! Be Happy, Keep Smiling!'}]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = predict(pred_sentences)\n",
        "predictions"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "sentiment-analysis.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
