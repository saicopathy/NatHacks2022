{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load Our Packages\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:982\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1423\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1392\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1356\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[0;34m(cls, path)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted"
     ]
    }
   ],
   "source": [
    "# Load Our Packages\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tables import split_type\n",
    "from insight_stress_analysis import *\n",
    "\n",
    "# Load spacy pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_noun_generator(redditcontentstring)\n",
    "\n",
    "    nouns = [ token.text for token in redditcontentstring if token.is_stop != True and token.is_punct !=True and token.pos_ == 'NOUN']\n",
    "    # nouns\n",
    "\n",
    "    word_freq = Counter(nouns)\n",
    "    common_nouns = word_freq.most_common(100)\n",
    "    return common_nouns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_verb(token):\n",
    "    \"\"\"Check verb type given spacy token\"\"\"\n",
    "    if token.pos_ == 'VERB':\n",
    "        indirect_object = False\n",
    "        direct_object = False\n",
    "        for item in token.children:\n",
    "            if(item.dep_ == \"iobj\" or item.dep_ == \"pobj\"):\n",
    "                indirect_object = True\n",
    "            if (item.dep_ == \"dobj\" or item.dep_ == \"dative\"):\n",
    "                direct_object = True\n",
    "        if indirect_object and direct_object:\n",
    "            return 'DITRANVERB'\n",
    "        elif direct_object and not indirect_object:\n",
    "            return 'TRANVERB'\n",
    "        elif not direct_object and not indirect_object:\n",
    "            return 'INTRANVERB'\n",
    "        else:\n",
    "            return 'VERB'\n",
    "    else:\n",
    "        return token.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt for user to input a journal entry\n",
    "journal = input(\"Journal your feelings and vent about a time you were stressed\")\n",
    "print(\"Journal entry: \" + journal)\n",
    "#creating txt file for the user input\n",
    "\n",
    "y.write(journal)\n",
    "y=open(r'C:\\Users\\gurmo\\OneDrive\\Desktop\\Sait 2022\\nathacks 2022\\Doc1.txt', 'w')\n",
    "\n",
    "redditcontentstring = nlp(open(file_name, 'r', encoding='cp1252').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_verb(redditcontentstring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dad', 3), ('morning', 2), ('cousin', 2), ('work', 1), ('conversation', 1), ('argument', 1), ('reason', 1), ('house', 1), ('uncle', 1), ('tray', 1), ('breakfast', 1), ('hug', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(common_noun_generator(redditcontentstring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" PUNCT punct\n",
      "I PRON nsubj\n",
      "never ADV neg\n",
      "see VERB ccomp\n",
      "my PRON poss\n",
      "dad NOUN dobj\n",
      "often ADV advmod\n",
      "because SCONJ mark\n",
      "he PRON nsubj\n",
      "’s VERB advcl\n",
      "always ADV advmod\n",
      "at ADP prep\n",
      "work NOUN pobj\n",
      "or CCONJ cc\n",
      "I PRON nsubj\n",
      "might AUX aux\n",
      "see VERB conj\n",
      "him PRON dobj\n",
      "in ADP prep\n",
      "the DET det\n",
      "morning NOUN pobj\n",
      "but CCONJ cc\n",
      "we PRON nsubj\n",
      "’d NOUN appos\n",
      "would AUX aux\n",
      "n’t PART neg\n",
      "talk VERB conj\n",
      ", PUNCT punct\n",
      "but CCONJ cc\n",
      "my PRON poss\n",
      "dad NOUN nsubj\n",
      "never ADV neg\n",
      "supported VERB conj\n",
      "me PRON dobj\n",
      "in ADP prep\n",
      "what PRON dobj\n",
      "I PRON nsubj\n",
      "want VERB pcomp\n",
      "to PART aux\n",
      "do VERB xcomp\n",
      "or CCONJ cc\n",
      "what PRON dobj\n",
      "I PRON nsubj\n",
      "did VERB conj\n",
      ", PUNCT punct\n",
      "he PRON nsubj\n",
      "would AUX aux\n",
      "always ADV advmod\n",
      "say VERB ccomp\n",
      "“ PUNCT punct\n",
      "Ok INTJ intj\n",
      ", PUNCT punct\n",
      "do VERB xcomp\n",
      "better ADJ advmod\n",
      "” PUNCT punct\n",
      "and CCONJ cc\n",
      "I PRON nsubj\n",
      "ca AUX aux\n",
      "n’t PART neg\n",
      "even ADV advmod\n",
      "have VERB conj\n",
      "a DET det\n",
      "proper ADJ amod\n",
      "conversation NOUN dobj\n",
      "with ADP prep\n",
      "him PRON pobj\n",
      "with ADP prep\n",
      "yelling VERB pcomp\n",
      "or CCONJ cc\n",
      "having VERB conj\n",
      "an DET det\n",
      "argument NOUN dobj\n",
      "with ADP prep\n",
      "him PRON pobj\n",
      ", PUNCT punct\n",
      "and CCONJ cc\n",
      "he PRON nsubj\n",
      "would AUX aux\n",
      "yell VERB conj\n",
      "at ADP prep\n",
      "me PRON pobj\n",
      "for ADP prep\n",
      "no DET det\n",
      "reason NOUN pobj\n",
      ", PUNCT punct\n",
      "I PRON nsubj\n",
      "remember VERB ROOT\n",
      "once SCONJ mark\n",
      "I PRON nsubj\n",
      "went VERB ccomp\n",
      "to ADP prep\n",
      "my PRON poss\n",
      "cousin NOUN compound\n",
      "’s PART compound\n",
      "house NOUN pobj\n",
      "and CCONJ cc\n",
      "I PRON nsubj\n",
      "saw VERB conj\n",
      "my PRON poss\n",
      "uncle NOUN nsubj\n",
      "having VERB ccomp\n",
      "a DET det\n",
      "tray NOUN dobj\n",
      "of ADP prep\n",
      "homemade ADJ amod\n",
      "breakfast NOUN pobj\n",
      "for ADP prep\n",
      "my PRON poss\n",
      "cousin NOUN pobj\n",
      "and CCONJ cc\n",
      "even ADV advmod\n",
      "brought VERB conj\n",
      "it PRON dobj\n",
      "to ADP prep\n",
      "her PRON pobj\n",
      "and CCONJ cc\n",
      "told VERB conj\n",
      "her PRON poss\n",
      "good ADJ amod\n",
      "morning NOUN dobj\n",
      "and CCONJ cc\n",
      "gave VERB conj\n",
      "her PRON dative\n",
      "a DET det\n",
      "hug NOUN dobj\n",
      ", PUNCT punct\n",
      "and CCONJ cc\n",
      "she PRON nsubj\n",
      "’s VERB conj\n",
      "like INTJ intj\n",
      "18 NUM npadvmod\n",
      "and CCONJ cc\n",
      "like INTJ intj\n",
      "when SCONJ advmod\n",
      "I PRON nsubj\n",
      "saw VERB advcl\n",
      "that SCONJ mark\n",
      "it PRON nsubj\n",
      "just ADV advmod\n",
      "kinda ADV advmod\n",
      "made VERB ccomp\n",
      "me PRON nsubj\n",
      "really ADV advmod\n",
      "sad ADJ ccomp\n",
      "inside ADP prep\n",
      "that SCONJ mark\n",
      "my PRON poss\n",
      "dad NOUN nsubj\n",
      "would AUX aux\n",
      "never ADV neg\n",
      "do VERB ccomp\n",
      "that PRON dobj\n",
      "for ADP prep\n",
      "me PRON pobj\n",
      "\" PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "redditcontentstring = nlp(open('Doc1.txt', 'r', encoding='cp1252').read())\n",
    "for token in redditcontentstring:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "#code for entities\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "redditcontentstring = nlp(open('Doc1.txt', 'r', encoding='cp1252').read())\n",
    "\n",
    "for ent in redditcontentstring.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(journal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fad1b179a91d41a0b14d3d47b729614d24c4bbfd77e9848bc70857acbaefee90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
