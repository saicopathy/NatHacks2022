{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal your feelings and vent about a time you were stressedhj\n",
      "Journal entry: hj\n"
     ]
    }
   ],
   "source": [
    "#prompt for user to input a journal entry\n",
    "journal = input(\"Journal your feelings and vent about a time you were stressed\")\n",
    "print(\"Journal entry: \" + journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I never see my dad often because he’s always at work or I might see him in the morning but we’d wouldn’t talk, but my dad never supported me in what I want to do or what I did, he would always say “Ok, do better” and I can’t even have a proper conversation with him with yelling or having an argument with him, and he would yell at me for no reason, I remember once I went to my cousin’s house and I saw my uncle having a tray of homemade breakfast for my cousin and even brought it to her and told her good morning and gave her a hug, and she’s like 18 and like when I saw that it just kinda made me really sad inside that my dad would never do that for me'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text file \n",
    "\"I never see my dad often because he’s always at work or I might see him in the morning but we’d wouldn’t talk, but my dad never supported me in what I want to do or what I did, he would always say “Ok, do better” and I can’t even have a proper conversation with him with yelling or having an argument with him, and he would yell at me for no reason, I remember once I went to my cousin’s house and I saw my uncle having a tray of homemade breakfast for my cousin and even brought it to her and told her good morning and gave her a hug, and she’s like 18 and like when I saw that it just kinda made me really sad inside that my dad would never do that for me\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal your feelings and vent about a time you were stressedsdfghn\n",
      "Journal entry: sdfghn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#prompt for user to input a journal entry\n",
    "journal = input(\"Journal your feelings and vent about a time you were stressed\")\n",
    "print(\"Journal entry: \" + journal)\n",
    "\n",
    "#creating txt file for the user input\n",
    "y=open(r'C:\\Users\\gurmo\\OneDrive\\Desktop\\Sait 2022\\nathacks 2022\\Doc1.txt', 'w')\n",
    "y.write(journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_type\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#nlp = spacy.load('en')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m  \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load spacy pipeline\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# nlp = spacy.load(\"en_core_web_sm\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m  \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load spacy pipeline\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# redditcontent = nlp(open('MigraineText.txt', 'r', encoding='utf-8'))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# redditcontentstring = nlp(redditcontent.read())\u001b[39;00m\n\u001b[1;32m     17\u001b[0m redditcontentstring \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoc1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp1252\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:436\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Load Our Packages\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    " \n",
    "from tables import split_type\n",
    "#nlp = spacy.load('en')\n",
    " \n",
    "# Load spacy pipeline\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    " \n",
    "# Load spacy pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    " \n",
    "# redditcontent = nlp(open('MigraineText.txt', 'r', encoding='utf-8'))\n",
    "# redditcontentstring = nlp(redditcontent.read())\n",
    "redditcontentstring = nlp(open('Doc1.txt', 'r', encoding='cp1252').read())\n",
    "# docx\n",
    "#print(redditcontentstring)\n",
    " \n",
    "nouns = [ token.text for token in redditcontentstring if token.is_stop != True and token.is_punct !=True and token.pos_ == 'NOUN']\n",
    "# nouns\n",
    " \n",
    "word_freq = Counter(nouns)\n",
    "common_nouns = word_freq.most_common(100)\n",
    "print(common_nouns)\n",
    " \n",
    "# Export to CSV\n",
    "#df = pd.DataFrame(common_nouns)\n",
    "# df.columns = [Input,'Sentence', 'Aspect', 'Descriptive Term', 'Polarity', 'Subjectivity']\n",
    "#df.columns = [\"WordPosts\",\"WordPostCounts\"]\n",
    "#df.to_csv('kneepainCount.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "redditcontentstring = nlp(open('Doc1.txt', 'r', encoding='cp1252').read())\n",
    "for token in redditcontentstring:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "#code for entities\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "redditcontentstring = nlp(open('Doc1.txt', 'r', encoding='cp1252').read())\n",
    "\n",
    "for ent in redditcontentstring.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I never see my dad often because he’s always at work or I might see him in the morning but we’d wouldn’t talk, but my dad never supported me in what I want to do or what I did, he would always say “Ok, do better” and I can’t even have a proper conversation with him with yelling or having an argument with him, and he would yell at me for no reason, I remember once I went to my cousin’s house and I saw my uncle having a tray of homemade breakfast for my cousin and even brought it to her and told her good morning and gave her a hug, and she’s like 18 and like when I saw that it just kinda made me really sad inside that my dad would never do that for me\"\n"
     ]
    }
   ],
   "source": [
    "# Load Our Packages\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    " \n",
    "from tables import split_type\n",
    "#nlp = spacy.load('en')\n",
    " \n",
    "# Load spacy pipeline\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    " \n",
    "# Load spacy pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    " \n",
    "# redditcontent = nlp(open('MigraineText.txt', 'r', encoding='utf-8'))\n",
    "# redditcontentstring = nlp(redditcontent.read())\n",
    "redditcontentstring = nlp(open('Doc1.txt', 'r', encoding='cp1252').read())\n",
    "\n",
    "tokens = redditcontentstring\n",
    "\n",
    "def check_verb(token):\n",
    "    \"\"\"Check verb type given spacy token\"\"\"\n",
    "    if token.pos_ == 'VERB':\n",
    "        indirect_object = False\n",
    "        direct_object = False\n",
    "        for item in token.children:\n",
    "            if(item.dep_ == \"iobj\" or item.dep_ == \"pobj\"):\n",
    "                indirect_object = True\n",
    "            if (item.dep_ == \"dobj\" or item.dep_ == \"dative\"):\n",
    "                direct_object = True\n",
    "        if indirect_object and direct_object:\n",
    "            return 'DITRANVERB'\n",
    "        elif direct_object and not indirect_object:\n",
    "            return 'TRANVERB'\n",
    "        elif not direct_object and not indirect_object:\n",
    "            return 'INTRANVERB'\n",
    "        else:\n",
    "            return 'VERB'\n",
    "    else:\n",
    "        return token.pos_\n",
    "\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fad1b179a91d41a0b14d3d47b729614d24c4bbfd77e9848bc70857acbaefee90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
